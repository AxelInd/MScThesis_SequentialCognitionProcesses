\chapter{Cognitive Operations: The Cognitive Toolbox} \label{chp:toolbox}
%reference that paper that shows the improvements in machine learning
\section{Overview}
As discussed in Section~\ref{ssec:intu}, and illustrated in Figure~\ref{fig:scp_general} the simplest imaginable description of sequential human cognition consists of a single step transforming the initial task information (along with background information) into an epistemic state that can be externally interpreted to choose a response. That is, $\mu=(\pi=(S_i \longmapsto m),f())$ can model any cognitive task with a complex enough definition of $m$.

To understand why this is poor choice of SCP, consider a general limitation of the SCP framework: we can only change the output of an SCP by changing the initial epistemic state, or by inserting/deleting a cognitive operation at an existing state point. Consider a case where the initial epistemic state is immutable. Thus, for $f(x \longmapsto A)$, the only possible changes are: $f(x \longmapsto \bar{B} \longmapsto A \longmapsto \bar{B})$, $f(x \longmapsto A \longmapsto \bar{B})$, $f(x \longmapsto \bar{B} \longmapsto A)$, $f(x \longmapsto \bar{B})$, and $f(x)$. Where $\bar{B}$ denotes the action of drawing some sequence of random, but valid cognitive state transitions. There is no way to change the operations that occur in $A$. Thus, even though Figure~\ref{fig:scp_general} may be an appropriate model for the general reasoner of a given task, it imposes the implicit assumption that all other test subjects who achieved differing results, achieved them only by either completely ignoring the black-box process the general reasoners used, or else by inserting new cognitive operations only at the very start or end of the processing task. In Section~\ref{sec:supSCP} and Section~\ref{sec:wstSCP} we will explore examples of cases where assumptions like that are simply not reflected by empirical data. 

Instead of using a single black-box operation to describe all cognition, it follows logically to use multiple smaller black box operations, each taking a state point as input and resulting in a state point as output. Using this interpretation, cognitive operations for the general reasoner should be as extensive as possible, provided that they do not force implausible changes to describe deviant users or when used in other cognitive tasks. Figure~\ref{fig:scp_general} and Figure~\ref{fig:sfig2} in the previous chapter, captured this intuition as it applies to the WCS. Indeed, as a non-monotonic logic that is already explicitly divided into discrete steps in the literature Section~\ref{ssec:wcs}, its individual steps are obvious candidate cognitive operations.

It is worth noting that this increased ability to model deviant reasoners comes at the cost of forcing more state points into the SCP being used.

The remainder of this section is devoted to expressing the exact properties, limitations, and concrete descriptions of cognitive operations, as well as defining several well-founded cognitive operations algorithmically with respect to their preconditions, effects, and output properties in Section~\ref{sec:pso}.

\section{Cognitive Operation Space}
%then SCP space is cog op space size ^^ scp length
In principle, the number of cognitive operations available for use in an SCP is unlimited. We denote the space of all cognitive operations $\Omega$, and the space of all pCTMs $(m_0 \in \Omega \longmapsto ... \longmapsto m_n \in \Omega)$ with $\Omega^*$. Similarly, given the SCP Task $\Pi=(s_i,M,f() \gamma)$, $M$ denotes the set of allowable cognitive operations for that task and $M^*=\{(m_1 \longmapsto ... \longmapsto m_n| m_i \in M)\}$ is the space of all pCTMs for that task. There cannot and never will be an exhaustive search procedure to evaluate every possible way of manipulating an epistemic state (Proofs~\ref{proof:infiniteSCPs} and \ref{proof:infiniteSCPLength} illustrate the existence of valid SCPs for infinitely many different cognitive operations and valid SCPs of any length, respectively). However, there are ways of limiting the number of allowable cognitive operations. Definition~\ref{lem:uniredundant} defines those cases in which there is redundancy in the set of all possible cognitive operations, Definition~\ref{lem:taskredundant} focuses this concept on specific cognitive tasks which are to be modelled using SCPs for a given SCP Task $\Pi$, redundant cognitive operations can be removed from the set of allowable cognitive operations $M$ without changing the domain of $\{f(\pi)|\pi$ is a valid SCP of $\Pi\}$.


\section{A Limited Set of Allowable Operations}
In practice it is impossible compute the set of all candidate SCPs when the set of allowable cognitive operations $M$ in a planning task is $\Omega^*$ or even $\Omega$. Without a length constraint Proof~\ref{proof:infiniteSCPLength} shows that sometimes it is  not even possible to compute all possible SCPs for many SCP tasks with a finite set of allowable operations. Table~\ref{tbl:scpSpace} summarises the properties of solution space under different SCP task conditions.

\begin{table}
\begin{center}

\begin{tabular}{ c | c c c}
 \textbf{$M$} & \textbf{Finite Length Constraint} & \textbf{Finite Solution Space} & \textbf{Guaranteed Solution}\\ 
 \hline
 $\Omega^*$ & None & \text{\sffamily X} & \checkmark \\ 
 $\Omega^*$ & Yes & \text{\sffamily X} & \checkmark \\ 
 $\Omega$ & None & \text{\sffamily X} & \checkmark \\ 
 $\Omega$ & Yes & \text{\sffamily X} & \checkmark \\  
 finite &  None & not guaranteed & \text{\sffamily X}\\  
 finite &  Yes &  \checkmark & \text{\sffamily X}
\end{tabular}
\caption{Solution space considerations of SCPs tasks as determined by length constraints and choice of cognitive operations.}
\label{tbl:scpSpace}

\end{center}
\end{table}

\subsection{A Limited Set of Cognitive Operations}\label{ssec:limCogOp}
Readers should recognize that using the space of all possible cognitive operations for $M$ results in a computationally impossible task. No matter the other conditions imposed there will be infinitely many solutions to the given SCP task which satisfy $f()$.

But, beyond being just computationally infeasible, there are more empirical and philosophical objections to this approach. The first is that the human brain itself does not have infinite processing or space capabilities and so any model which allows for unrestricted data sizes in the set of allowable cognitive operations $M$ or in the number of resultant state points $M$ is not biologically well-founded\footnote{Because $\Omega$ is the set of all possible cognitive operations there must be at least one $m \in M$ such that, for any given input $s_i$, $J[s_i,m]$ results in a state point of information content greater than the capacity of any storage system to hold.}. 

The second consideration is that there is considerable evidence in the literature that many problems can be modelled using logical frameworks which describe cognitive behaviour across a variety of tasks -- for example, the Diversity Principle\citep{heit2005defending}. These frameworks reflect a more generally philosophy in cognitive psychology which is the idea of consistent cognitive motifs in reasoning. Using an infinite $M$ shows no discrimination at the time of task-formulation between those operations for which there is significant evidence and those which are fundamentally improbable.

\subsection{A Limited SCP Length}
As discussed in Section~\ref{ssec:limCogOp}, the human brain is constrained by a set of physical, computational, and storage limitations. In all cases except those which are trivial or repetitive it would be computationally infeasible for the human mind to mimic an arbitrarily long SCP. For this reason, SCPs of infinite length, though possible in a mathematical formulation, tend to violate the spirit of the SCP Framework which is to \textit{accurately} model human cognition processes, not simply their conclusions.

It is worth noting that SCPs containing infinite loops can exist and can be evaluated at some time points $n$, they simply cannot be evaluated in their entirety. For example, the SCP $\mu=((s_i\longmapsto A),f())$, if the definition of cognitive operation $A$ includes a call to $A$ every time it is evaluated. 

\section{Purpose-Selected Cognitive Operations} \label{sec:pso}
\subsection{Overview}

This section discusses the intuition and precise definition of several cognitive operations which will be used throughout the remainder of this paper. These operations allow the SCP Framework to mimic non-monotonic logics for the experiments discussed in Chapter~\ref{chp:model}.

A cognitive operation $m$ is always evaluated in terms of each base point $\bar{p} \in p$ of the input state point, and each base point can be evaluated independently of the others. For these definitions, a base point $\bar{p}$ is assumed as input and a state point is returned as output. Function $\texttt{functionName}_\chi(\bar{p})$ determines the validity of the input base points. $\Sigma_\chi$ is called the input alphabet and defines the minimal set of an variables names which must be present in $\bar{p}$ for the operation to be allowable.

Accessing a specific element $\alpha_k$ in the structural variables of base point $\bar{p}=(\alpha_1,..,\alpha_n)$ is short-handed to $\bar{p}[\alpha_k]$. $\bar{p}[\alpha_k]$ returns $\alpha_k$ if and only if $\alpha_k \in \bar{p}$ and returns the empty set otherwise. This indexing notation is also used to append a structural variable to $\bar{p}$ and $\bar{p}[\alpha_j]:=t$ removes $\bar{p}[\alpha_j]$ from $\bar{p}$ if it already exists and creates a new structural variable $\alpha_j=t$ in the resulting state point $p'$.

\subsection{Preconditions}

The input alphabet $\Sigma_\chi=\{\alpha_1, ..., \alpha_k\}$ of a cognitive function $m$ describes the set of structural variables which must be present in the epistemic states $\bar{p} \in_S p$ of the input state point $p$. The alphabet of structural variables in $p$ is given by $\Sigma_p$. Hybrid validity, as defined in Section~\ref{ssec:validity}, can be algorithmically illustrated as follows:

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$\Sigma_{\chi}$ ($\bar{p}$)}
{
\tcc{Where each structural variable $\alpha_i$ must be present in $p$ to validate the precondition.}
$\Sigma_{\chi}=\{\alpha_1,...,\alpha_n\}$\;
\eIf{$\forall v \in \Sigma_\chi, v \in \Sigma_p$}
{
\Return ``Defined"
}
{
\Return ``Undefined"
}
}

\caption{$\texttt{functionName}_\chi(\bar{p})$): Hybrid validity requirements for $J[p,m]$ to be defined.}
\label{cogOp:precon}
\end{algorithm}

All cognitive operations which we define in this section will be assumed to have a known $\Sigma_\chi$ and will only proceed to their effect $e$ if $\texttt{functionName}_\chi(\bar{p})=$ ``Defined''.
The output state point of a defined $J[\bar{p},m]$ is the returned value of $m[e](\bar{p})$.

\subsection{Propositional Logic}
%no requirement of sequence, just exhaustive search
%not computationaly valid, NP-complete is too hard for human brain
An SCP for evaluating propositional logic programs, as defined in Section~\ref{ssec:propLog}, must be able to take a set of rules and facts and draw the set of all valid inferences from that set using the standard propositional rules discussed in Section~\ref{ssec:propLog}. To that purpose, we introduce a cognitive operation called \texttt{th} which draws all classically valid inferences from a set of logical rules\footnote{The reason that we do not introduce the functionality of the \texttt{th} cognition operation in the external evaluation function $f()$ is that doing so would make it impossible to directly use the derived theorems as part of another SCP.}.

\texttt{th} is by its nature a contrived cognitive operation and there is no expectation that it truly reflects human cognition. Instead, it serves as a benchmark operation with which to compare the results derived from knowledge base-driven logical approaches. Even a simpler congitive operation called \texttt{SAT} which determines if the given knowledge base it satisfiable still represents an \textit{NP-complete}  problem and is infeasible for any large knowledge base\citep{schaefer1978complexity}.

However, variations on the intuition of these two problems have real applications in cognitive modelling with SCPs and will be discussed throughout this section.

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
$\bar{p}[\textrm{`S'}]:=\bar{p}[\textrm{`S'}] \cup [(\text{atom name} \leftarrow \text{value}) |$ ($\text{atom name},\text{value}) \in \bar{p}[\textrm{`V'}]$ and $\text{value} \neq u$\;
$\bar{p}[\textrm{`S'}]:=\bar{p}[\textrm{`S'}] \cup [A$ | clause $A$ would not make $\bar{p}[\textrm{`S'}]$ inconsistent$]$\;
$\bar{p}[\textrm{`V'}]:=[B$ | $B \leftarrow body \in \bar{p}[\textrm{`S'}]$ and $ I(body)=\top$ or $I(body)=\bot$]\; 
\Return $\bar{p}$
}

\caption{\texttt{th}$(\bar{p})$: generates the potentially infinite set of possible classical inferences from $\bar{p}[\textrm{`S'}]$.}
\label{cogOp:th}
\end{algorithm}

A realised SCP $r=(k=K[\pi],f())$ is said to \textit{model the classical inference}, if $f(k) = f(s_i \longmapsto \texttt{th})$, where $s_i$ is the initial state point of the CTM $\pi$. An SCP $\pi$ is said to model the classical inference if some realised SCP generated from $\pi$ models the classical inference.


\subsection{Variable Insertion}

Variable insertion is a single cognitive operation with a highly intuitive interpretation and justification. By its nature, the set of variables $\bar{p}[text{`V'}]$ in an epistemic state $\bar{p}$ is not a true possible world (as discussed in Section~\ref{ssec:epiprops}), and may omit variables present in the real possible world. But one these variables may become apparent later. For example, when interpreting a conditional as a license for implication, or when storing the result of a calculated mathematical problem.

It seems reasonable then that a cognitive operation $\texttt{addV}$ which appends a variable to base point $V$ would be required to expand the knowledge base to incorporate new facts.

\begin{algorithm}[H] 
\SetAlgoLined
\tcc{The variable to be added, defined before SCP execution.}
$v:= \alpha=(\text{atom name},\text{value})$\;
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
\tcc{Remove all possible assignments of the variable from the list of variables}
$\bar{p}[\textrm{`V'}]:=\bar{p}[\textrm{`V'}] - [\text{atom name}, x \in \text{Domain}_\text{atom name}] $
\tcc{Add the new variable and its assignment to the list of variables}
$\bar{p}[\textrm{`V'}]:=\bar{p}[\textrm{`V'}] \cup \alpha$\;
\Return $\bar{p}$
}

\caption{\texttt{addV}$(\bar{p})$: adds a variable $v$, defined \textit{a priori}}
\label{cogOp:addV}
\end{algorithm}

In order to ensure that the SCP still acts as a pipeline of information, it is necessary that $\texttt{Add}\alpha$ for variable $\alpha=(\text{atom name, value})$ is defined as part of the allowable operations of the SCP Task $\Pi$ which generates the SCPs and realised SCPs that will make use of it. The core principle of this pipeline framework is that each usable component (cognitive operation) of that framework which is not the input value should not be modified.

\subsection{Variable Deletion} \label{ssec:deletion}
Following on from the $\texttt{addV}$ operation, the $\texttt{remove}$ operations captures human intuition of forgetting. Consider the sequence of numbers: 1, 44, 27, 8, 0 , -4, 6, 7, 346, 7, 74, 7, 234, -55, 2.4, 18. Now without looking back at the numbers, ask yourself some questions: how many numbers were there? Were any of them prime? How many numbers were repeated? In all probability you are not entirely sure. This simple thought experiment provides support for our first extension, the idea that variables can be ``forgotten", that is, that information that existed in the knowledge base at one point in time might no longer exist at a later timepoint. 

This is not the only imaginable case where a variable might be removed from the knowledge base of the person being modelled. The size of the knowledge base used for cognitive modelling is always implicitly restricted to relevant variables. Only those variables whose values might reasonably be expected to affect the final conclusions drawn with regard to the research question should be considered. Finding which variables and rules are relevant is, however, non-trivial. For another real-life example, imagine a mystery novel: Three hundred pages of plot descriptions, character actions, and dialogues. In a good murder mystery novel every piece of information that reveals the killer's identity is hidden in the story itself, yet we do not hold every fact and interaction in the book in our epistemic model of the book, so discerning the identity of the killer remains a mystery until the last page. But when the mystery is solved, many details that we internalised while reading (and recall in retrospect) suddenly make the conclusion seem obvious. We have not forgotten this information, we had merely incorrectly deemed it irrelevant at the time and ignored it in our cognitive processing.

Forgetting a variable in an SCP is not-trivial task, just as removing a variable in an AI planning task is non-trivial. Multiple intuitions for how it may best be done occur, but we will keep this definition as simple as possible for the examples that will be discussed in Chapter~\ref{chp:model}. We introduce three cognitive operations $\texttt{remove}_\top$, $\texttt{remove}_\bot$, and $\texttt{remove}_u$. $\texttt{remove}_\top$ assumes that conditionals and rules which depend on the removed variable are validated by its removal. $\texttt{remove}_\bot$ assumes that rules and conditionals are invalidated by the removal. $\texttt{remove}_u$ assumes that no conclusion can be drawn without this information.  These three approaches can be differentiated by these examples:

\begin{itemize}
\item ``If aliens invade, then I will hide in my bunker."
\item ``If I do not fail my exam, then I will graduate.''
\item ``Unless I am a boy, I am a girl.''
\end{itemize}

In the first example, the presumed default position is that I will not hide in my bunker. In the second, the default position is that I will graduate. And in the third example there is no apparent default position. In the absence of the conditions in each each of these conditionals we would probably conclude that, I am probably not hiding in my bunker, I probably graduated, and that my gender is uncertain. This observation implies that a mechanism by which to determine the plausibility of default positions is needed to translate such default theories into a non-monotonic framework. In the absence of such a mechanism we will generally assume $\texttt{remove}_\bot$, as with the closed-world assumption.

The variables to be deleted are specified in categorization variable at $\bar{p}[\text{`R'}][\text{`delete'}]$ for a given base point $\bar{p}$. If a variable does not exist, no change occurs.

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
$\bar{p}'=\texttt{clone}(\bar{p})$\;
$\text{toDel}:=\bar{p}'[\text{`R'}][\text{`delete'}]$\;
\For{ $\text{varName} \in \text{toDel}$}
{
Remove $\textit{varName}$ from the possible world variable $\bar{p}'[\text{`V'}]$.\;
$\bar{p}':=\texttt{rem}(\bar{p}', \textit{varName})$\;
}
\Return $\bar{p}'$
}

\Fn{$\texttt{rem}(\bar{p},v)$}
{
\For{$\text{rule}=(\text{head} \leftarrow \text{body}) \in \bar{p}[\textrm{`S'}]$}
{
\If{$\text{head}=v$}
{
$\bar{p}[\textrm{`S'}]:=\bar{p}[\textrm{`S'}]-\text{rule}$\;
}
\If{$\text{v} \in \text{body}$}
{
replace all subclauses $(v \lor \phi)$, $(v \land \phi)\in \bar{p}[\textrm{`S'}]$ with $(\phi)$\;
}
}
\For{$\text{rule}=(\text{head}|\text{body}) \in \bar{p}[\textrm{`}\Delta\textrm{'}]$}
{
\If{$\text{head}=v$}
{
$\bar{p}[\textrm{`}\Delta\textrm{'}]:=\bar{p}[\textrm{`}\Delta\textrm{'}] - \text{rule}$\;
}
\If{$\text{v} \in \text{body}$}
{
replace all subclauses $(v \lor \phi)$, $(v \land \phi)\in \bar{p}[\textrm{`}\Delta\textrm{'}]$ with $(\phi)$\;
}
}
\tcc{This is where $\texttt{remove}_\bot$ differs from $\texttt{remove}_\top$, and  $\texttt{remove}_u$ }
replace all rules $(\text{head} \leftarrow v) \in \bar{p}[\textrm{`S'}]$ with $\bot$\;
replace all rules $(\text{head} \leftarrow \lnot v) \in \bar{p}[\textrm{`S'}]$ with $\lnot \bot$\;
replace all rules $(\text{head}|v)$ with $\bot$\;
replace all rules $(\text{head}|\lnot v)$ with $\lnot \bot$\;

\Return $\bar{p}$
}

\caption{\texttt{remove}$_\bot(\bar{p})$: removes a variable name $v$, defined \textit{a priori}}
\label{cogOp:removeuV}
\end{algorithm}
\subsection{Variable Fixation} \label{ssec:variableFixing}
The next case of a potential complex operation to add to the search space of our SCPs is the idea of Variable Fixing. The idea that some  conclusions can be fixed \textit{a priori}. This operation may prove contentious to those who are experienced with mathematical logic, but holds some merit when justified from a psychological perspective. 

Consider a person who strongly doubts the effectiveness of vaccines, we will call her Karen. Karen started her day convinced that giving her child the MMR vaccine is more dangerous than the disease itself. Later that day Karen spoke to her doctor who strongly advised that she vaccinate her child. He offered her a variety of peer-reviewed papers and studies that showed the relative safety of the vaccination. Karen listened carefully to the trained medical professional, and then went home. After some thought Karen decided that he was wrong, and her opinion on vaccines didn't change.

In this example Karen shows a very powerful type of cognitive bias, the unwillingness to change her opinions, despite powerful evidence to the contrary. This phenomenon has been observed across a great many fields of study, from medical psychology\citep{brown2010omission}\citep{wroe2005feeling} to political sciences\citep{tappin2017heart}. In the context of cognitive modelling with logics, it indicates that some mental rules or variables are immutable, regardless of new evidence or valid beliefs that would logically contradict them. Non-monotonic logics, as a class, are already capable of dealing with bias effects, as non-monotonic logics are built on the basis of a preference operation.

In order to implement this operation, we will make use of the categorization variable $R$ first discussed in Section~\ref{ssec:epiprops}.

\begin{algorithm}[H]
\SetAlgoLined
\tcc{The variable to be fixed, defined before SCP execution.}
$v:= (\alpha = (\text{atom name}, \text{value}))$\;
\SetKwProg{Fn}{Function}{ is}{end}

\Fn{$e$($\bar{p}$)}
{
$\bar{p}[\textrm{`V'}]:=\bar{p}[\textrm{`V'}] - (\text{atom name}, \text{value} \in \text{domain}) \in \bar{p}[\textrm{`V'}]) \cup \alpha$\;
$\bar{p}[\textrm{`R'}][\text{`fixed'}]:= \bar{p}[\textrm{`R'}][\text{`fixed'}] \cup v$\;
\Return $\bar{p}$
}

\caption{\texttt{FixV}$(\bar{p})$: fixes a variable name $v$, defined \textit{a priori}}
 \label{cogOp:fixV}
\end{algorithm}


This algorithm removes `$\text{atom name}$' from the possible world $V$ and readds it with the value to which it is to be fixed. The atom is then added to the list of fixed variables kept by $\bar{p}[\textrm{`R'}]$. This list of fixed variables is used by other operations which evaluate state points later in the SCP or realised SCP, and, as per their encoding, they will not modify the assignment of this variable.

\subsection{Adding Abnormalities}

As discussed in Section~\ref{ssec:condInterpretation}, one way to interpret conditionals in reasoning tasks is as licenses for implication. One possible definition for a cognitive operation that performs this same task by transforming defeasible conditional facts to propositional logic rules is as given by Algorithm~\ref{cogOp:addAB}.

\begin{algorithm}[H]
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
\tcc{Variables for which an abnormality has already been created}
\For{$(\psi|\phi) \in \bar{p}[\textrm{`}\Delta]\textrm{'}$}
{
$k:=$ the lowest natural number for which $\text{ab}_k \notin \bar{p}[\textrm{`S'}]$\;
$\text{all dependencies}:= [A | (\psi|A) \in \bar{p}[\textrm{`}\Delta\textrm{'}]]$\;

\For{$A \in \text{all dependencies}$}
{
$\text{head}:=\psi$\;
\tcc{All other variables which can make $\psi$ true are treated as though they could also falsify $\psi$.}
$\text{current dependencies}:= \text{all dependencies} - \text{A}$\;
\If{$\text{current dependencies} = \{\}$}
{
$\text{current dependencies}:=\bot$\;
}
$\text{body}:=(\text{current dependencies}_1 \lor ... \lor \text{current dependencies}_n)$\;

\tcc{Add the conditional as a license for implication to the set of rules.}
$\bar{p}[\textrm{`S'}]:= \bar{p}[\textrm{`S'}] \cup (\text{head} \leftarrow A \land \lnot \text{ab}_k)$\;
$\bar{p}[\textrm{`S'}]:= \text{ab}_k \leftarrow \lnot body$\;

}
\tcc{Add the new abnormality to the possible world $V$.}
$\bar{p}[\textrm{`V'}]:= \bar{p}[\textrm{`V'}] \cup (\text{ab}_k,u)$\;
}
\tcc{Remove all conditionals now that they have been interpreted as licences for implication.}
$\Delta:=\{\}$\;
\Return $\bar{p}$
}
\caption{\texttt{addAB}$(\bar{p})$}
 \label{cogOp:addAB}
\end{algorithm}

\subsection{Weakly Completing}


Weak completion is an essential part of the WCS. Under the assumption that the WCS is a valid representation of human cognition in at least one scenario, any comprehensive set of cognitive operations $M$ must be able to mimic the Weak Completion of a knowledge base. Algorithm~\ref{cogOp:wc} shows one way in which this can be achieved.

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
Replace all rules $\in \bar{p}[\textrm{`S'}]$ of the form $A\leftarrow \text{body}_1,...,A\leftarrow \text{body}_n$ with $A\leftarrow \text{body}_1 \lor ... \lor \text{body}_n$ \;
Replace all occurrences of $\leftarrow \in \bar{p}[\textrm{`S'}]$ with $\leftrightarrow$ \;
\Return $\bar{p}$
}

\caption{\texttt{wc}$(\bar{p})$}
\label{cogOp:wc}
\end{algorithm}



\subsection{Semantic Operator}

Another essential step in the WCS, the semantic operator is used to assign all variables to either True or False (explictly), or to Unknown (implicitly). Algorithm~\ref{cogOp:semantic} illustrates one way in which this cognitive operation can be implemented for any epistemic state of the form $s=(KB,V,...)$.

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
\tcc{All least models that can achieved by applying the semantic operator to the current state point.}
$p'= \{\phi_{SvL}(\bar{p})\}$\;
\Return $p'$
}

\Fn{$\phi_{SvL}$($\bar{p}$)}
{
\If{there exists a clause $A\leftarrow \text{body} \in \bar{p}[\textrm{`S'}]$ with $I_V(body)=\top$ }
{
\If { $\text{A} \not\in \bar{p}[\text{fixed}]$}
{
$\bar{p}[\textrm{`V'}][A]:=\top$
}
}
\If{there exists a clause $A\leftarrow \text{body} \in \bar{p}[\textrm{`S'}]$ and for all clauses $A\leftarrow \text{body} \in \bar{p}[\textrm{`S'}]$ we find $I_V(body)=\bot$}
{
\If { $\text{A} \not\in \bar{p}[\textrm{`fixed'}]$}
{
$\bar{p}[\textrm{`V'}][A]:=\bot$
}
}
\Return $\bar{p}$
}

\caption{\texttt{semantic}$(\bar{p})$}
\label{cogOp:semantic}
\end{algorithm}

\subsection{Adding Abducibles} \label{ssec:abd}
Next, we define a cognitive operation which uses the set of possible abducibles, given by $\bar{p}['R']['abducibles']$. We have already used the intuition of this operation to handle the WST and derive the general answer set. Later we will see that it can also be used to model individual reasoners in the Suppression Task. \texttt{AddExp} adds possible explanations $\epsilon$ to the set of rules $S$ in the epistemic state and is defined as follows:

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e(\bar{p})$}
{
abducibles:=$\bar{p}[\textrm{`abducibles'}]$\;
$p':=[]$\;
\For{every unique subset $A \in$ abducibles }
{
newP := $\bar{p}$\;
$\text{newP}[\textrm{'S'}]=\text{newP}[\textrm{`S'}] \cup \{a \in A\leftarrow \top\}$\;
$\text{newP}[\textrm{'}\epsilon \textrm{'}]=A$\;
$p':=p' \cup \text{newP}$\;
}
\Return $p'$
}

\caption{$\texttt{addExp}$}
\label{cogOp:addExp}
\end{algorithm}


\subsection{Default Inferencing}
If we assume that Reiter's default logic is a valid model of cognition for at least one task, it follows that an comprehensive formulation of $M$ must encode a cognitive operation for drawing inferences from a set of default rules. Algorithm~\ref{cogOp:di} describes the procedure for drawing such inferences from an epistemic state state of the form $s=(W,D)$ where $W$ is a deductively closed set of inference rules, and $D$ is a set of default rules of the form $\frac{\text{pre}(\delta):\text{just}(\delta)}{\text{cons}(\delta)}$.

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
Let $\textrm{theory}=(W,D)$, where $W=\bar{p}[\text{`W'}]$, and $D=\bar{p}[\textrm{`D'}]$\;
Determine $\Pi$, the set of all closed, successful default processes for default theory $(W,D)$\;
\Return \texttt{createP}($\bar{p}$,$\Pi$)
}
\Fn{\texttt{createP}($\bar{p}, \Pi$)}
{
$p'=[]$
\For{$\pi \in \Pi$}
{
$\bar{p}':=\texttt{clone}(\bar{p})$\;
$\bar{p}'[\textrm{`W'}]:=\bar{p}'[W]\cup \text{In}(\pi)$\;
\tcc{If no default processes were previously applied, simply add the default process.}
$\bar{p}'[\textrm{`R'}][\textrm{`}\delta\textrm{'}]:=\bar{p}'[\textrm{`R'}][\textrm{`}\delta\textrm{'}]\cup \delta$\;
Add any inferences of the form $\text{head}\leftarrow \text{body}$, where head is an atomic name, and body is a truth value, as the pair $(\text{head},\text{body})$ to $\bar{p}'[\textrm{`V'}]$\;
$p':=p'\cup \bar{p}'$\;
\Return $p'$
}
}

\caption{\texttt{default}$(\bar{p})$: encodes the inference of valid default processes.}
\label{cogOp:di}
\end{algorithm}

As with the $\texttt{th}$ cognitive operation, $\texttt{default}$ is only a practically useful function if some approximation of the set of deductively closed inferences can be generated or evaluated.

\section{Cognitive Operations as Aggregates}
%talk about combining existing cognitive operations for which there is strong evidence
Any valid pCTM can be combined into a single cognitive operation (Proof~\ref{proof:aggregateValid}). This leads to an obvious and interesting idea. That of finding well-founded, uninterrupted epistemic operation sequences which are effective in modelling a variety of tasks and representing them as a single epistemic operation. We call this approach \textit{aggregating}, and it draws inspiration from recent advances in the field of reinforcement learning\citep{drummond2002accelerating} in which simple sequences of actions which solve recurring subtasks across multiple solutions in the learning domain are treated a new, combined, and callable action when solving future problems in that domain. 

This approach introduces the desirable property of shortening the total length of the SCP for a given set of cognitive tasks. However, it is evident from Proof~\ref{proof:aggregateExpressiveness} that any SCP task in which an aggregated subsequence of operations replaces those individual operations in $M$ may be less expressive than the same formulation in which the aggregated operations are still present.

For example, given SCP Task $\Pi=(s_i,M,f(),\gamma)$, with $\texttt{wc} \in M$ and $\texttt{semantic}\in M$, the cognitive processes {\texttt{wc} and \texttt{semantic} could be combined together to form the pCTM $\texttt{wcs}=(\texttt{wc} \longmapsto \texttt{semantic})$. If we restrict solution SCPs for $\Pi$ to those which contain $\texttt{wcs}$, then we guarantee that first the weak completion operation, and then the semantic operation must be applied in that order and with no other cognitive operations occurring between them. This approach may lead to fewer satisfying SCPs, but the found solution may generally be more plausible than those which are discounted.

The use of aggregates then, is a trade-off between sacrificing cross-reasoner accuracy and optimising a set of desirable heuristic properties such as length minimisation or limiting the number of state points in the final state if using a final state dependent SCP. It is worth noting that any valid pCTM can be treated as a single valid cognitive operation when using a final state dependent evaluation function without affecting the domain of the SCP Task.

In many ways this compromise follows the philosophy of cognitive modelling in general. A neuron-by-neuron approach to predicting human behaviour (if one were ever possible) would give us extreme accuracy in modelling any human reasoner, but is too complex to be practical, and even if it were, would provide no abstracted information with which to find common motifs and inferences among reasoners. On the other extreme, modelling reasoners as (\textit{input}, \textit{output}) pairs perfectly captures the average predictions of a population, but proves very inaccurate for unseen individuals. State-of-the-art approaches to cognitive modelling like neural networks and non-monotonic logical frameworks seek generalizations which accurately capture the responses of most reasoners across as many tasks as possible by approximating human motifs in human reasoning and applying across multiple tasks and inputs.

A researcher could decide that it is unreasonable to expect any cognitive operations to seperate \texttt{wc} and \texttt{semantic}, as they both form part of the same nonmonotonic logic. They might then create the new aggregate complex operation \texttt{wcs} which applies the WCS, and, thus, guarantees a unique model, in its entirety as follows:

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
$\bar{p}':=\texttt{wc}(\bar{p})$\;
$p':=\texttt{semantic}(\bar{p'})$\;
\Return $p'$
}

\caption{\texttt{wcs}$(\bar{p})$}
\label{cogOp:wcs}
\end{algorithm}

It is worth noting that, when $\texttt{wcs} \in M$, and $\texttt{wc} \not\in M$, $\texttt{semantic}\not\in M$, some satisfying SCPs no longer exist. For example, it is possible to model the general case of the Suppression by only applying the semantic operator, and not weakly completing (this solution does not guarantee a unique least model, but does correctly model the task). Aggregate cognitive operations reduce the SCP space that needs to be explored to find satisfying SCPs and can enforce properties like the unique least model, but often do so at the cost of obscuring other solutions.

Researchers could even go a step further and define the cognitive operation $\texttt{abwcs}$ which first interprets the conditionals in $p[\textrm{`}\Delta\textrm{'}]$ and then applies the WCS to the result. This algorithm is as follows:

\begin{algorithm}[H] 
\SetAlgoLined
\SetKwProg{Fn}{Function}{ is}{end}
\Fn{$e$($\bar{p}$)}
{
$\bar{p}':=\texttt{addAB}(\bar{p})$\;
$\bar{p}':=\texttt{wc}(\bar{p})$\;
$p':=\texttt{semantic}(\bar{p'})$\;
\Return $p'$
}

\caption{\texttt{abwcs}$(\bar{p})$}
\label{cogOp:abwcs}
\end{algorithm}

The reader will note that $\texttt{abwcs}$ now provides precisely the same functionality as the examples of the WCS we examined in Chapter~\ref{chp:experiments}, and Chapter~\ref{chp:model} will show that these three operations in sequence can indeed be modelled in the SCP framework and come to the same conclusions as when the WCS is applied in the original experiments.

The question of how to generate appropriate complex operation aggregates is a complex problem and is not discussed further in this thesis. It is, however a topic in which the author has significant interest.


\section{Epistemic State Structure Changes with Cognitive Operations}
%e.g. going to WCS to Default
The concept and mathematics of cognitive operations whose output states points which differ in structure from their input state point structure are fairly straightforward. Some cognitive operation $A \in \Omega^*$ with input base point structure $\textrm{struct}(s_k)$ and output base point structure $\textrm{struct}(s_{k+1})$ is called a \textit{structural transformation operation} if and only if $\textrm{struct}(s_k) \ne \textrm{struct}(s_{k+1})$.

The intuition behind structural state changes goes back to Albert Einstein's quote that opened this thesis ``Everything should be made as simple as possible, but no simpler.". In theory there is no restriction on SCP structure that would prevent every epistemic state passing an arbitrarily large number of variables and arguments. An SCP meant to represent a cognitive task where participant responses are consistent with propositional logic, for example, could be accurately represented with an initial state $s_x=(S)$ where $S$ is simply the knowledge base of facts and rules given; but a researcher might equally use an epistemic state $s_y=(S,V)$ where a variable $V$ is intended to store the values of each variable in $S$; or even $s_z=(S,V,D)$ where $D$ is a set of default rules. All three of these approaches would give an SCP the information required to evaluate the propositional task, but, if we know the task structure does not deal with uncertainty or show variation in reasoner responses, it becomes apparent that the set of default rules $D$ is unnecessary.

All of the possibilities given hold for the propositional task because $s_y$ and $s_z$ are both supersets of $s_x$ and $s_x$ is sufficient to model the task. Thus, provided the researchers do not believe that the task will make use of cognitive operations which require more complex state structures, $s_x$ is the simplest solution which meets the requirements of the researchers.

An obvious extension to the discussion above is the fact that a transformation from one state point structure to another when the input structure is a subset of the output structure is as easy as appending more structural variables to the input state. In the example above a propositional logic compliant state point $s_x$ could be transformed into a WCS compliant state point $s_y$ or a default rule compliant state point $s_z$.

Unfortunately, structural changes caused by appending variables are not the only type of structural changes. Obvious cases of structural transformation where they do not apply spring to mind. Such as the case where a more expressive state point must be changed to a less expressive state point structure. Imagine a case where a cognitive operation $B$ transforms a case point of $\textrm{struct}(s_z)$ to one of $\textrm{struct}(s_y)$. An intuitive example of this might be a hypothetical mental operation which handles biases in thinking. A student might begin at an epistemic state which believes ``usually, if I study for tests, I fail" and, through bias confirmation, come to believe ``If i study for tests, I fail", transitioning from a default rule, to immutable rule. 

In cases like this, it is up to the researcher to make several decisions: do I believe that the now empty set of variables $D$ no longer provides meaningful information? And how do I represent the change in the variables still present in the output state? These questions are very often task or function specific and cannot be answered in a general sense.

\section{Two Approaches to Creating Cognitive Operations}
\subsection{Theoretical Approximation}
%must have empirical basis, but focused on reproducing mechanisms that have been proposed and substantiated in literature
%hand-curated
It seems reasonable that, given our desire to accurately model reasoning in participants, researchers creating cognitive functions for SCPs would choose to limit the set of allowable cognitive operations to those for which there is a sound theoretical basis.

The first approach to creating cognitive operations we will consider is the use of cognitive operations for which there is already evidence in our existing models of human cognition. This evidence can take a wide variety of forms: it may follow from our knowledge of physics (for example, the impossibility of storing a set of unrelated variables above a certain physical threshold); evolutionary biology (there cannot exist a class of organisms without some kind of reproductive drive); anatomy (there must exist cognitive processes that facilitate voluntary muscle activation); sociology (some neuronal connections seem to restrict the number of close friends a human can maintain \citep{gonccalves2011modeling}); or any other field of science. Using this information can allow researchers to predict what cognitive functions may play a role in observed empirical data. 

For the most part, theoretical approximation is a very powerful and well-justified mechanism for determining which cognitive operation could plausibly considered in the cognitive toolbox for a given task. Restricting the space of other cognitive operations which may play a part is also enabled by all the scientific fields mentioned above, complexity theory tells us that humans are unlikely to apply exhaustive classical logic reasoning to a task when the set of variables and rules to be considered is large because of physical limitations related to solving \textit{NP-complete} or harder tasks efficiently.

At present, the foundation of non-monotonic reasoning in cognitive modelling is this theoretical mindset, a well-founded explanation is found by borrowing from a relevent field (often psychology) and then tested against empirical evidence and across multiple cognitive tasks to show evidence that is a reasonable approximation of a mechanism in human cognition.

This approach was used throughout this chapter and is analogous to the argument made by \cite{stenning2012human} that modelling reasoning should be done primarily towards an appropriate theoretical representation, and then secondarily towards approximating empirical reasons

\subsection{Empirical Appoximation}
%finding cognitive operations using machine learning
%done by using search techniques to create and explore comparison space
Contrasting theoretical approximation is the field of constrained empirical approximation. Techniques ranging from neural networks and Markov models to genetically inspired evolutionary algorithms consistently outperform humans in a huge variety of fields \citep{saad2004information}. Drawing on enormous databases of empirical data, modern artificial intelligence researchers have come to view machine learning as the most plausible way to create the first real thinking machine.

\textit{pseudo-cognitive operations} are a class of cognitive operation which meet the mathematical definition of cognitive operations, but do not have either theoretical or empirical support for its existence in human cognition. Arguably, almost every cognitive operation in $\Omega$ and aggregated operation in $\Omega^*$ meets this definition. The operation $\texttt{th}$ which outputs an epistemic state containing the deductive closure of its input states is one example of a pseudo-cognitive operation. 

By utilizing a large set of simplistic pseudo-cognitive operations and using reinforcement learning techniques such as subtask learning \citep{drummond2002accelerating}, it may well be possible to generate complex aggregate operations from these simple building blocks. And aggregate operations which are shown to be viable across different cognitive tasks will have strong empirical, but not theoretical, support for their existence as motifs in human reasoning.


















